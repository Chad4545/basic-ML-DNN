{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Activation Function ?\n",
    "\n",
    "- NN에서 뉴런에 들어오는 입력신호의 총합(Weighted sum)을 출력신호로 변환하는 함수\n",
    "\n",
    "- 신경망의 출력 끝에 추가하는 것 == 일종의 노드(node)\n",
    "\n",
    "- 또는 Transfer function이라 부름\n",
    "\n",
    "-  활성화함수는 대개 비선형함수(non-linear function)을 사용\n",
    "\n",
    "       선형함수를 사용한다면, layer를 쌓는 의미가 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Activation Function\n",
    "\n",
    "## Sigmoid or Logistic Activation Function\n",
    "\n",
    "- sigmoid\n",
    "\n",
    "- output range : 0~1\n",
    "\n",
    "<img src='sig.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sigmoid derivative graph\n",
    "\n",
    "<img src='sig2.png'>\n",
    "<img src='sig3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 장점\n",
    "\n",
    "- output : 0~1 이기 때문에  output으로 확률을 구하는 모델에서 주로 쓰인다. (Binary classification에서도 물론)\n",
    "\n",
    "- 또한 미분가능하며, S자 커브 형태로 자연스럽게 활성화를 해주기 때문에 NN에서 많이 쓰였음(지금은 별로 안쓰나봄)\n",
    "\n",
    "#### 단점\n",
    "\n",
    "- 시그모이드 함수 자체는 monotonic 그러나 미분값의 그래프는 그렇지 않다.\n",
    "\n",
    "- 이는, 입력값이 -3보다 작거나 3보다 클 경우 gradient 값이 지나치게 작아지는 문제를 발생(학습이 잘 안됨) 또한 학습속도도 느려짐\n",
    "\n",
    "  (If input is too small or large, activation value is easy to be saturated)\n",
    "  \n",
    "  \n",
    "- 함수값이 zero-centered 가 아님. 입력값이 항상 양일때 w에 대한 함수 미분값이 항상 양이거나 음이 됨. \n",
    "\n",
    "  X, W가 2차원 벡터라고 가정하면, W의 gradient는 2,4 사분면에 존재할수 없음 -> 학습에 좋지않음\n",
    "  \n",
    "<img src='sig4.png'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh or hyperbolic tangent Activation Function\n",
    "\n",
    "- Tanh : 시그모이드 함수의 크기와 위치를 조절(rescale and shift)한 함수\n",
    "\n",
    "- output range : -1~1\n",
    "<img src='tanh2.png'>\n",
    "<img src='tanh.png'>\n",
    "\n",
    "\n",
    "- Tanh derivative graph\n",
    "\n",
    "<img src='tanh5.png'>\n",
    "<img src='tanh4.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 장점\n",
    "- 시그모이드 함수와는 달리 0을 기준으로 대칭\n",
    "  \n",
    "         negative input이 negative로 mapping되고 zero input이 tanh 그래프에서 0에 mapping\n",
    "         \n",
    "- 또한, sigmoid에 비해 gradient값이 크다 \n",
    "         \n",
    "- 따라서 sigmoid 보다는 학습수렴속도가 빠르다\n",
    "\n",
    "<img src='tanh6.png'>\n",
    "\n",
    "\n",
    "\n",
    "#### 단점\n",
    "\n",
    "- 여전히 vanishing gradient 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU (Rectified Linear Unit) Activation Function\n",
    "\n",
    "- ReLU : x 가 양수이기만 하면 그래디언트가 1로 일정하므로 Vanishing gradient 문제 해결\n",
    "\n",
    "         미분하기도 편리해 계산복잡성이 낮음\n",
    "         \n",
    "- output range : [ 0 to infinity)\n",
    "\n",
    "<img src='relu.png'>\n",
    "\n",
    "#### 장점\n",
    "\n",
    "\n",
    "- 함수값이 saturation 문제 없음\n",
    "\n",
    "- 계산이 빠름\n",
    "\n",
    "#### 단점\n",
    "\n",
    "- input 값이 음수인 경우, gradient가 0이 되는 약점 있음. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "- Leaky ReLU : ReLU의 단점을 보완\n",
    "\n",
    "   \n",
    "         \n",
    "- output range : [ 0 to infinity)\n",
    "\n",
    "<img src='leaky.png'>\n",
    "\n",
    "- input값이  음수일 때, gradient가 0.01이라는 점을 제외하고는 ReLU와 같은 특성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
