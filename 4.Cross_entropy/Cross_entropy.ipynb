{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- https://youtu.be/uMYhthKw1PU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification의 Loss function으로는 CE를 주로 사용합니다.\n",
    "\n",
    "# 두 분포의 차이를 줄이기 위해 KL-divergence를 최소화 합니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "- 이와 같은 말을 너무나도 많이 접하게 되고, 그냥 자연스럽게 그런가보다 하고 넘어가게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 과연 이게 뭘까..?\n",
    "\n",
    "## Cross_entropy\n",
    "\n",
    "- 일반적으로, softmax의 loss 함수로써 Cross_entropy를 쓴다.\n",
    "\n",
    "- 과연 이게 뭘까 ?\n",
    "\n",
    "--- \n",
    "\n",
    "- Logistic Loss == Multinomial Logistic Loss == Cross-Entropy loss\n",
    "- 주로, multi-class classification task의 loss로 쓰인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='ce1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### entropy short review\n",
    "\n",
    "<img src='ce2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- yi는 확률, log(1/yi)는 information gain\n",
    "\n",
    "- information gain번역하자면 정보획득량이라고 할 수 있을겁니다.\n",
    " \n",
    "- 이해를 돕기위해, 예를 들어 설명해 보겠습니다.\n",
    "\n",
    "      여러분이 형사에요. 용의자가 김씨라는걸 알았어요.\n",
    "      반면에, 엄씨라는걸 알았어요.\n",
    "      엄씨는 굉장히 드물잖아요 -> '엄씨' 정보를 얻었을때 얻는 정보량이 '김씨'정보를 얻었을때 보다 많다.\n",
    "      (한국인의 1/3은 '김씨' -> 실질적인 정보는 거의 없다. 반면에 '엄씨' 자체가 희귀하다 -> 얻는 정보가 많다.)\n",
    "\n",
    "- 우리가 얻는 정보라는 것은 , 그 정보의 희귀성과 비례 == 확률에 반비례 -> 역수를 취해 log 취해줌\n",
    "-  이것을 information  gain이라고 합니다. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다시 Cross_Entropy\n",
    "\n",
    "<img src='ce3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞에서, entropy 정보량을 최적으로 압축시켰을때 나오는 bit 수 라고 정리했음.\n",
    "\n",
    "- 그런데 만약에 우리가 최적이 아니라 좀 잘못된 정보(불확실한 정보)를 사용했다고 가정하자 ( -> predict값 = q(x) )\n",
    "\n",
    "- 그래서 information gain을 잘못된 정보를 기반으로해서 계산했다고 하면, \n",
    "\n",
    "- 정보량은 최적의 bit수 보다는 약간 더 크겠죠 ? ( 잘못된 정보를 기반으로해서 계산을 했기 때문에, 최적으로 압축불가)\n",
    "\n",
    "- 그래서 최적의 정보량인 entropy보다 약간 큰 정보량을 지닌,  잘못된 정보를 사용해서 얻은\n",
    "\n",
    "- 정보량을 cross entropy라고 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 갑자기 KL-Divergence\n",
    "\n",
    "<img src='ce4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### KL Divergence \n",
    " \n",
    " - 이게 뭐냐면, distance같은 하지만 distance는 아닌 하지만 distance로 쓰고 싶은애라고 한다..\n",
    " \n",
    "   사실 Divergence의 개념이 아예 없어서 본인은 설명이 불가능하다.\n",
    "   \n",
    "  \n",
    " - 아무튼 probability situation에서 p확률분포와 q확률분포가 있을때,  이 두 확률분포간의 거리를 재고 싶을떄, 이 KL Divergence를 많이 쓴다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - cross entropy, entropy와 연결해서 설명하자면\n",
    " - 바로 그 최적이 아닌 인코딩(cross entropy)과 최적의 인코딩(entropy)의 차이를 KL-Divergence라고 합니다.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src='ce5.png'>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL 식을 보시면, H(P)는 Q에 관한 함수가 아니기 때문에 상수 취급이 가능합니다.\n",
    "- 일반적으로 Q(잘못된 정보, 우리의 predict)를 P (Target)에 가깝게 Optimize하는데, \n",
    "- 이 작업에 대해서 H(P)가 아무런 역할을 하지 못하기 때문에 H(P)를 생략 가능하고, Optimize를 진행해도 됩니다.\n",
    "- 따라서, CE를 MInimize 하나 KL-divergence를 minimize하나 똑같은 결과를 냅니다.\n",
    "\n",
    "---\n",
    "- 그래서 True가 P이고 Predict가 Q인 경우, CE를 objective로 하든 KL을 objective로 하든 상관 없지만(이론적으로)\n",
    "\n",
    "- 주로, CE를 loss function으로 사용하게 됩니다.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
