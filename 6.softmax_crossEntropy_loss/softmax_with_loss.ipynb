{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "-https://kakalabblog.wordpress.com\n",
    "\n",
    "-https://ratsgo.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax_with_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiclass classification를 풀기 위한 딥러닝 모델 말단엔 softmax 함수가 적용됩니다. \n",
    "\n",
    "- softmax 함수는 class 수만큼의 차원을 갖는 입력벡터를 받아서 확률(요소의 합이 1)로 변환해 줍니다. \n",
    "\n",
    "- 이후 손실 함수로는 크로스엔트로피(cross entropy)가 쓰입니다\n",
    "\n",
    "- 크로스엔트로피는 소프트맥스 확률의 분포와 정답 분포와의 차이를 나타냅니다. \n",
    "\n",
    "- 이를 기본으로 해서 손실(오차)을 최소화하는 방향으로 모델의 각 파라메터를 업데이트하는 과정이 바로 딥러닝 모델의 학습이 되겠습니다.\n",
    "\n",
    "\n",
    "- 아래는 기본적인 softmax_with_loss를 도식화한 그림입니다.\n",
    "\n",
    "<img src='swl.png'>\n",
    "\n",
    "- a를 입력으로 받아서 소프트맥스를 취해 확률값으로 만든 뒤, 이를 바탕으로 크로스 엔트로피 Loss L을 출력합니다. \n",
    "\n",
    "- 반대로 역전파하는 그래디언트는 yk−tk가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax\n",
    "- 분류해야 할 class 수가 n이고 softmax 함수의 i번째 입력값을 ai, i번째 출력값을 pi라고 할 때 \n",
    "\n",
    "  (소프트맥스 함수의 입력 및 출력벡터의 차원수는 n이 됩니다.)\n",
    "<img src='swl2.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross-entropy as Loss\n",
    "- 아래에서 yj는 정답 벡터의 j번째 요소라는 뜻입니다. \n",
    "\n",
    "- 3 classes 분류를 하는 문제에서 어떤 데이터의 정답이 첫번째 class라면 y=[1,0,0]이 되고, y1=1, 나머지 y2,y3은 0이 됩니다. \n",
    "\n",
    "- pj는 소프트맥스 함수의 j번째 출력값입니다.\n",
    "\n",
    "  (loss fuction의 output은 스칼라 값입니다. 그리고 이 값을 Minimize하는 것이 우리의 목표 입니다.)\n",
    "<img src='swl3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of Softmax w.r.t  input ai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='swl4.png'>\n",
    "<img src='swl5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of Loss(Cross_entropy) w.r.t ith input ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='swl6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  loss에 대한 i번째 input값 ai의 gradient는 결국\n",
    "\n",
    "- (추정값 - 정답), scalr꼴이 됩니다.\n",
    "\n",
    "- gradient를 구하기가 매우 쉽고, 이렇게 구한 gradient 또한 0으로 죽는 일이 많지 않아서 softmax + CE 조합이 많이  쓰이는 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of Loss(MSE) w.r.t ith input ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='swl7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss 함수로 MSE를 쓴다면, 다음과 같이 Gradient계산이 복잡해지는 단점이 있습니다.(여전히 softmax의 Gradient term을 계산해야함)\n",
    "- 또한, MSE특성상 오차를 제곱하기 때문에, 오차의 절대량도 커지게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
