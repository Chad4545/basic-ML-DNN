{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- http://sanghyukchun.github.io\n",
    "- https://arxiv.org/abs/1502.03167\n",
    "- https://youtu.be/TDx8iZHwFtM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "- Batch Normalization은 기본적으로 Gradient Vanishing / Gradient Exploding 이 일어나지 않도록 하는 아이디어 중의 하나\n",
    "\n",
    "- 기존의 해결방향\n",
    "    - Activation 함수의 변화 (ReLU 등)\n",
    "    - Initialization -> difficult\n",
    "    - small learning rate -> slow & local minimum\n",
    "    \n",
    "- batch normalization은 이러한 간접적인 방법보다는 training 하는 과정 자체를 전체적으로 안정화하여 학습 속도를 증가시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존의 문제점 \n",
    "<img src='batch1.png'>\n",
    "\n",
    "- NN은 파라미터가 많아서 학습이 어렵다\n",
    "- DNN 같은경우는 레이어가 깊어짐에 따라 parameter가 더 많아져서 학습이 더 어렵다\n",
    "- 왜 힘드냐 -> weight의 조그마한 변화가 가중되서 쌓이면 (= hidden layer가 깊어질수록 )더 큰 변화를 가져오기 때문 \n",
    "- 그래서 기존의 값과 다른 hidden layer값, output을 가지게 될 것임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal Covariate Shift\n",
    "- 이러한 문제점을 Internal Covariate Shift 라 부름 \n",
    "<img src='batch2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적인 Machine learning AlgorithmsTrain set과 test Set의 분포가 다르면 학습이 안되는 것과 같이..\n",
    "\n",
    "- hidden layer의 node들의 변동이 심하다 == Internal Covariate Shift\n",
    "\n",
    "- 만약, 변동이 작아서 h2까지 안정적인 노드값을 가져간다면 학습이 잘 될것\n",
    "\n",
    "- 하지만, 변동이 크다면, 기존의 값(앞선 batch으 data)과 너무 다르기 때문에 어떻게 학습할지 모르게됨 \n",
    "\n",
    "- DNN에서도 , 처음에 학습한 layer 노드들의 분포와 그 다음에 학습하는 분포가 너무 다르다면 학습이 안될것 \n",
    "<img src='batch3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NN의 Forward learning 부분\n",
    "- Input 에서 Hidden layer 로 전달될 떄, \n",
    "- Wx 와 같이 weight sum을 해준 후, 그 값이 Activation function의 input으로 들어가게 되고\n",
    "- 최종적으로 그 다음 layer에 전달됨\n",
    "<img src='batch4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization 적용\n",
    "\n",
    "- 문제되는 부분은 Wx임 (변동이 생기는 부분)\n",
    "\n",
    "- 하지만 batch norm 을 써서 이러한 변화를 줄이는것을 목표로 함 \n",
    "\n",
    "- weight sum이 batch norm에 들어오면 그것의 scale을 변화시켜줌(normalization)\n",
    "\n",
    "- batch norm을 쓰면 기존의 학습했던 node값들(이전 batch)의 분포와 크게 다르지 않은 분포(이번 batch의)를 갖게 됨 \n",
    "\n",
    "<img src='batch5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인풋에서 히든갈때 W 곱을해주고 activation을 먹이고 H로 넘어감\n",
    "- 문제되는 부분은 Wx임  이렇게 변함\n",
    "\n",
    "- 하지만 batch norm 을 써서 이러한 변화를 줄이는것을 목표로 함 \n",
    "\n",
    "- 그래서 weight sum이 batch norm에 들어오면 그것의 scale을 변화시켜줌(normalization)으로 준다 .\n",
    "\n",
    "- batch norm을 쓰면 기존의 학습했던 node값들의 분포와 크게 다르지 않은 분포를 갖게 됨 \n",
    "\n",
    "<img src='batch8.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-  알고리즘의 개요는 다음과 같다.\n",
    "\n",
    "- Batch size에 따라  각 feature 별로 평균과 표준편차를 구해준 다음 normalize 해주고,\n",
    "\n",
    "- scale factor와 shift factor를 이용하여 새로운 값을 만들어준다.\n",
    "\n",
    "\n",
    "<img src='batch.png'>\n",
    "\n",
    "- 여기서  scale and shift를 해주는 이유는 \n",
    "\n",
    "- activation function에 따라 결과가 달라지기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를 들어, Activation function이 Relu인 경우\n",
    "\n",
    "- normalized해준 range에서, backpropagation 진행 시 많은 node들이 죽어버리는 경우가 생기고\n",
    "\n",
    "<img src='batch9.png'>\n",
    "\n",
    "- 예를 들어, Activation function이 sigmoid인 경우\n",
    "\n",
    "- normalized해준 range에서, Non-linearity를 취하자는 목적과 달리 linear한 부분만 취하게 된다.\n",
    "\n",
    "<img src='batch10.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Batch norm을 사용하면 Internal Covariate Shift가 감소하여\n",
    "\n",
    "- 보다 높은 Learning-rate를 적용하여 학습속도를 높힐수 있고,\n",
    "\n",
    "        일반적으로 learning rate를 너무 높게 잡을 경우 gradient exploding/vanishing 문제 발생하거나, local minima 문제\n",
    "        이는, parameter들의 scale에서 기인하는데, batch norm은 parameter의 scale에 영향을 안받음\n",
    "\n",
    "- 초기 Initialize에 덜 민감하다.\n",
    "\n",
    "- 또한, Regularization 효과가 있어서 Drop out의 효과가 있다고 하는데 \n",
    "\n",
    "         batch size마다 각 batch에서 나오는 평균, 분산, 그리고 weight값이 다르기 때문에\n",
    "         determistic 하지 않고 stochastic하기 때문인듯 싶다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
