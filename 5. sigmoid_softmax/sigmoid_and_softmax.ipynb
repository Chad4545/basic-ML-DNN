{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://dataaspirant.com\n",
    "- https://youtu.be/aircAruvnKk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Sigmoid Fuction?\n",
    "\n",
    "<img src='sig.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sigmoid fuction\n",
    "      \n",
    "      Input: any range real number\n",
    "      output: value which falls in the range of 0 to 1\n",
    "      \n",
    "   \n",
    "-  시그모이드 함수는 다음과 같은 수식으로 표현되며, 주로 Logistic regression 모델안에서 Binary classification을 위해 쓰이거나\n",
    "- NN에서 Activation function으로 쓰입니다.\n",
    "- sigmoid의 반환 값이 확률형태(0~1)이기 때문에 결과를 확률로 해석할 때 유용하기 때문입니다.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Derivatives of sigmoid function\n",
    "\n",
    "<img src='sig3.png'>\n",
    "\n",
    "\n",
    "- 또한 시그모이드 함수를 1차 미분한 값은 다음과 같습니다.\n",
    "\n",
    "- input이 양극단값에 가까울수록 미분값이 0에 가까워집니다.\n",
    "\n",
    "- 이러한 특징은 Gradient Descent Algorithm 적용시 Vanishing Gradient Problem을 일으키는 이유 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='sig4.png'>\n",
    "\n",
    "- 또한 시그모이드 함수의 1차 미분은 다시 자기자신(1-자기자신)으로 표현된다는 점이 특징입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is Softmax Function?\n",
    "\n",
    "<img src='soft.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Softmax 함수는 'n'개의 다른 사건(label,class)에 대해 각 사건의 확률 분포를 계산합니다.\n",
    "\n",
    "- 일반적으로, 가능한 모든 대상 클래스에 대해 각 대상 클래스의 확률을 계산합니다.\n",
    "\n",
    "- 따라서, 계산 된 확률은 주어진 입력에 대한 Target class를 결정하는 데 도움이 됩니다.\n",
    "\n",
    "  (주로, logistic regression 모델안에서 multiple classification에 쓰입니다.)\n",
    "  \n",
    "- 또는, NN에서 Multiple classification 모델의 마지막 layer로 쓰입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='soft2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- z가 NN의 output이고, softmax로 output을 구현하면, 위 그림과 같이 최종 activation output이 상대적인 확률값을 갖게 됩니다.\n",
    "\n",
    "- 이는 softmax의 특징으로\n",
    "\n",
    "      계산된 확률은 0에서 1 이다.\n",
    "      모든 확률의 합은 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정리\n",
    "- Softmax: Used for the multi-classification task.\n",
    "- Sigmoid: Used for the binary classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='soft3.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
