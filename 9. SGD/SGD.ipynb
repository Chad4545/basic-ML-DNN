{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "- 최적해에 가까워지는 방향으로 조금씩 움직이면서 최적의 해를 찾는 최적화 알고리즘\n",
    "\n",
    "- 장님이 산 정상에서 가장 빠르게 내려오는 방법은 발 끝에서 느껴지는 경사가 가장 가파른 쪽으로 내려오는 것과 똑같은 이치\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "- 파란선 : 가중치에 대한 loss 함수\n",
    "- 검은점 : 현재 해의 위치\n",
    "- 화살표 : loss 함수를 최적화하기 위해 가중치가 움직여야할 방향\n",
    "<img src='gd.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "\n",
    "- $\\frac{\\partial}{\\partial_w}J(w)$ : gradient\n",
    "\n",
    "    weight에 대한 loss함수의 미분값\n",
    "    \n",
    "    미분 값의 부호는 최적해로 가기 위한 방향과 반대 값을 가짐\n",
    "    \n",
    "    \n",
    "-  $\\alpha$ : learning rate\n",
    "\n",
    "    움직이는 정도를 조정하는 계수\n",
    "    \n",
    "    클수록 빠르게 학습하지만, 최적해에 도달하지 못할 수도 있음\n",
    "\n",
    "<img src='weight.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "- 현재 위치에서 gradient를 구하고 그 반대방향으로 움직이는 과정을 반복하면서 최적 해를 찾아감\n",
    "\n",
    "\n",
    "<img src='gd1.png'>\n",
    "<img src='gd2.png'>\n",
    "<img src='gd3.png'>\n",
    "<img src='gd4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "- 다시말해서, loss 함수의 현 가중치의 기울기(gradient)를 구해서 loss를 줄이는 방향으로 업데이트 한다.\n",
    "\n",
    "- 하지만 loss를 구하기 위해서는 가진 모든 데이터를 넣어야지 전체 loss를 구할 수 있다.\n",
    "\n",
    "- 1번 업데이트 하는데 전체 데이터를 넣는다는 개념... -> 비효율적 + 다른 문제점 발생\n",
    "\n",
    "    - 학습시간증가\n",
    "    - over fitting\n",
    "    - 초기값을 어디로 잡냐에 따른 수렴 불가능성 등등 \n",
    "    - 또한 Duplicated Data의 존재 확률이 높음\n",
    "\n",
    "\n",
    "#### 만약에 훨씬 적은 계산으로 적절한 기울기를 얻을 수 있다면???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent(느린 완벽 보다 조금만 훑어보고 일단 빨리)\n",
    "\n",
    "- Data set에서 data를 ramdomly sampling 하면(it could be nosie there) 훨씬 적은 data set으로 중요한  \n",
    "\n",
    " 평균값을 추정할 수 있다\n",
    "\n",
    "- SGD은 이 아이디어를 더욱 확장한 것으로서, batch당 하나의 data(batch size=1)만을 사용. \n",
    "\n",
    "- batch : 모델 학습시 한 Iteration당 사용되는 데이터의 set\n",
    "\n",
    "- 즉, 데이터를 하나씩 추출하여 w를 업데이트. 이를 전체 data 수 만큼 반복 -> 오차율이 크다(속도는 빠르지만 global minima를 못찾을 가능성)\n",
    "\n",
    "<img src='t0.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch\n",
    "\n",
    "- Mini batch는 전체 Batch 반복(GD)과 SGD 간의 절충안\n",
    "\n",
    "- Mini batch는 일반적으로 random으로 선택한 10개에서 1,000개 사이의 Data로 구성.\n",
    "\n",
    "- mini batch SGD는 SGD의 noise를 줄이면서도 전체 batch(GD)보다는 더 효율적\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='t1.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
